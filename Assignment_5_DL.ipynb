{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhy would you want to use the Data API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. It work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat are the benefits of splitting a large dataset into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Data splitting is an important aspect of data science, particularly for creating models based on data. This technique helps ensure the creation of data models and processes that use data models such as machine learning and are accurate.\n",
    "Purpose of splitting data into the different category is also to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tDuring training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> We can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck.\n",
    "\n",
    "We can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches.\n",
    "\n",
    "If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. -----You can also try saving the dataset into multiple TFRecord files, and if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this).\n",
    "\n",
    "If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tCan you save any binary data to a TFRecord file, or only serialized protocol buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Tensorflow recommends to store and read data in tfRecords format. It internally uses Protocol Buffers to serialize/deserialize the data and store them in bytes, as it takes less space to hold an ample amount of data and to transfer them as well.\n",
    "\n",
    "Protobufs work with the predefined schemas, unlike JSON and XML. tfRecords has such schemas already available in Protofiles, and also the compiled code for many supported languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhy would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Protocol buffers are a method of serializing data that can be transmitted over wire or be stored in files. The other formats like JSON and XML are also used for serializing data. Although these platforms have proved themselves to be extremely flexible and effective, one place where they arenâ€™t fully optimized is scenarios where the data is to be transmitted between multiple microservices in a platform-neutral way.\n",
    "Like JSON and XML, the Protobufs are language and platform-neutral. The Protobuf is optimized to be faster than JSON and XML by removing many responsibilities usually done by data formats and making it focus only on the ability to serialize and deserialize data as fast as possible. Another important optimization is regarding how much network bandwidth is being utilized by making the transmitted data as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhen using TFRecords, when would you want to activate compression? Why not do it systematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->  If we export a table with arrays in the properties, we need to tell TensorFlow the shape of the array when it is read. A table exported to a TFRecord file will always be compressed with the GZIP compression type.We always get exactly one TFRecord file for each export. \n",
    "\n",
    "The TFRecord file is stored sequentially, enabling fast streaming due to low access times. And secondly, the TFRecord files are natively integrated into TensorFlows tf.data API, easily enabling batching, shuffling, caching, and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tData can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> It improves accuracy and reliability. Preprocessing data removes missing or inconsistent data values resulting from human or computer error, which can improve the accuracy and quality of a dataset, making it more reliable. It makes data consistent.\n",
    "Data Processing using tf.data pipeline\n",
    "Prefetching : The secretary will take and execute the treatment of the training stage before treatment. The model executes the training stage, but the input pipe reads the s + 1. This reduces the time required to extract the training and test \n",
    "Data Processing using TF Transform\n",
    "Data transformation is the process of converting, cleansing, and structuring data into a usable format that can be analyzed to support decision making processes, and to propel the growth of an organization. Data transformation is used when data needs to be converted to match that of the destination system.\n",
    "\n",
    "Data preprocessing problems can come in many flavors, but some of the most commons are:\n",
    "Missing data.\n",
    "Manual input.\n",
    "Data inconsistency.\n",
    "Regional formats.\n",
    "Numerical units.\n",
    "Wrong data types.\n",
    "File manipulation.\n",
    "Missing anonymization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
